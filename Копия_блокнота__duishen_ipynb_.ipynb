{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "LTL7PxqJmnN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator:\n",
        "    def __init__(self, model_name=\"ai-forever/mGPT-1.3B-kirgiz\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def generate(self, prompt, max_length=50, temperature=0.9):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True\n",
        "        )\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    def fine_tune(self, dataset, epochs=3, save_steps=500):\n",
        "        def tokenize_function(examples):\n",
        "            return self.tokenizer(examples[\"prompt\"], examples[\"completion\"],\n",
        "                               truncation=True, padding=\"max_length\",\n",
        "                               max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./fine_tuned_model\",\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=epochs,\n",
        "            per_device_train_batch_size=4,\n",
        "            save_steps=save_steps,\n",
        "            save_total_limit=2,\n",
        "            logging_dir=\"./logs\",\n",
        "            logging_steps=100,\n",
        "            learning_rate=5e-5,\n",
        "            warmup_steps=500,\n",
        "            weight_decay=0.01,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "        )\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "        trainer.save_model(\"./fine_tuned_model\")\n",
        "\n",
        "    def save_model(self, save_dir):\n",
        "        self.model.save_pretrained(save_dir)\n",
        "        self.tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "    @classmethod\n",
        "    def from_saved(cls, save_dir):\n",
        "        return cls(save_dir)"
      ],
      "metadata": {
        "id": "wyAu15dSmtq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessor:\n",
        "    @staticmethod\n",
        "    def load_and_split_data(file_path, num_words=8):\n",
        "        df = pd.read_csv(file_path, header=None, names=[\"full_text\"])\n",
        "        def split_text(text):\n",
        "            words = str(text).split()\n",
        "            prompt = \" \".join(words[:num_words])\n",
        "            completion = \" \".join(words[num_words:]) if len(words) > num_words else \"\"\n",
        "            return pd.Series([prompt, completion])\n",
        "\n",
        "        df[[\"prompt\", \"completion\"]] = df[\"full_text\"].apply(split_text)\n",
        "        return Dataset.from_pandas(df[[\"prompt\", \"completion\"]])"
      ],
      "metadata": {
        "id": "fomd0F1etSk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = DataPreprocessor()\n",
        "generator = TextGenerator()"
      ],
      "metadata": {
        "id": "7bU0SEbEtnDd",
        "outputId": "2af646e9-2d2e-40e4-f85f-ffda42b092f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = preprocessor.load_and_split_data(\"all_texts.txt\")"
      ],
      "metadata": {
        "id": "UFeCzVqv3w_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Начинаем дообучение модели...\")\n",
        "generator.fine_tune(dataset, epochs=3, save_steps=500)\n",
        "print(\"Дообучение завершено!\")"
      ],
      "metadata": {
        "id": "Du3QOdHfSnmm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6849868-fcae-4684-a70c-4f6838f4bbb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начинаем дообучение модели...\n",
            "Дообучение завершено!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Бүгүнкү аба ырайы\"\n",
        "output = generator.generate(prompt, max_length=500)\n",
        "print(\"\\nOutput:\", output)"
      ],
      "metadata": {
        "id": "XMTnT68r3ws4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5c22bb-789b-48f4-fee3-ac168a8591cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Output: Бүгүнкү аба ырайынын кескин өзгөрүшү кескин өсүп, андагы балдардын дагы эсеби өсүп жатат. Бүгүнкү күнү Бишкектеги жер титирөө катталганы маалым болду. Учурда Жалал-Абад шаары боюнча 10 балдар арасында жер титирөө болду.\n",
            "\n",
            "\n",
            "\n",
            "Хоккей | Прочие турниры | Тамбовское городское Дербис \"Арктика\"\n",
            "22.01.2018 18:00\n",
            "22.01.2018 18:00| Хоккей\n",
            "Шайба, ол., 25:00\n",
            "Плей-офф за 5-е место\n",
            "Шайба, ол., 22:00\n",
            "Шайба, ол., 18:00\n",
            "Плей-офф за 4-е место\n",
            "Шайба, ол., 17:00\n",
            "Шайба, ол., 15:00\n",
            "Плей-офф за 3-е место\n",
            "Плей-офф за 2-е место\n",
            "Нефтехимик, ол. (мол) - СКА, (мол) - ХК ААК ХК ЗХ\n",
            "Нефтехимик, ол., 17:00\n",
            "Рубин, ол. (мол) - Урал (мол), (мол) - ЦСКА, (мол) - ХК ААК ХК ЗХ\n",
            "\n",
            "\n",
            "\n",
            "﻿ ӨКМ: Кыргызстанда коронавирус жуктургандардын саны 668ге жетти - \"Кабар\" - ДЕМ\n",
            "Жарыяланган: 16.04.2020 @ 16:03\n",
            "Өлкөдө жугуштуу оорулардын алдын алуу программасы бир катар эл аралык уюмдардан, Жогорку Кеңештин, өкмөттүн\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Бүгүнкү аба ырайы\"\n",
        "output = generator.generate(prompt, max_length=64)\n",
        "print(\"\\nСгенерированный текст после дообучения:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "bRnT7fhuqU4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95720a5-ee58-46c6-f47d-6d1822bbf1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Сгенерированный текст после дообучения:\n",
            "Бүгүнкү аба ырайынын кооптуулугунан чыккан кыргыз жери кыйрап кетти. Бул шейшемби күнү айрым жарандар кыйнашты.\n",
            "Батыштан соккон шамалдын \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.save_model(\"./final_model\")\n",
        "print(\"Модель сохранена в папке 'final_model'\")"
      ],
      "metadata": {
        "id": "WfILCmF7VKfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b79518-1147-40c3-cf2f-e524f5ab0d9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель сохранена в папке 'final_model'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EsgHKM9xSxed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}